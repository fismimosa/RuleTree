{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "a1g6vLUtt9I9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba\n",
    "from numba import jit , njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8N0j1DSGt9I_"
   },
   "outputs": [],
   "source": [
    "# Global Config Variables\n",
    "n0 = 1000  # number of p=0 points in metric space\n",
    "V = n0  # Threshold for p=0\n",
    "K = 10# No of clusters\n",
    "A= 5# No of attributes\n",
    "iterations = 80  # maximum iteration in clustering\n",
    "runs =120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "l4AI4R6Nt9JA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from math import log2\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kqb5faS9t9JA"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import cm\n",
    "#from pyclustering.cluster.kmedians import kmedians\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rlaUUedgt9JB",
    "outputId": "787eb8a2-89ec-4014-f32b-e37db527ce82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn-extra\n",
      "  Obtaining dependency information for scikit-learn-extra from https://files.pythonhosted.org/packages/22/2f/86f58cb2bd3f81bdb28555470761c27c0d62a566329aac70e8507cb744fd/scikit_learn_extra-0.3.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading scikit_learn_extra-0.3.0-cp311-cp311-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\fedev\\anaconda3\\lib\\site-packages (from scikit-learn-extra) (1.24.3)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\fedev\\anaconda3\\lib\\site-packages (from scikit-learn-extra) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=0.23.0 in c:\\users\\fedev\\anaconda3\\lib\\site-packages (from scikit-learn-extra) (1.3.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\fedev\\anaconda3\\lib\\site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\fedev\\anaconda3\\lib\\site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (2.2.0)\n",
      "Downloading scikit_learn_extra-0.3.0-cp311-cp311-win_amd64.whl (340 kB)\n",
      "   ---------------------------------------- 0.0/340.5 kB ? eta -:--:--\n",
      "   ---- ---------------------------------- 41.0/340.5 kB 991.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  337.9/340.5 kB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 340.5/340.5 kB 4.2 MB/s eta 0:00:00\n",
      "Installing collected packages: scikit-learn-extra\n",
      "Successfully installed scikit-learn-extra-0.3.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "from sklearn.cluster import KMeans\n",
    "!pip install scikit-learn-extra\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from itertools import permutations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in directory:\n",
      "adult_p.csv\n",
      "bank-full_p_6col.csv\n",
      "bank-full_p_nodiv_6col.csv\n",
      "data_preprocess.ipynb\n",
      "diabetic_data_p.csv\n",
      "File 'adult_p.csv' is visible in the directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Specify the directory\n",
    "directory_path = r\"C:\\Users\\fedev\\Downloads\\ParTree-main\\ParTree-main\\Competitors\\algorithmic fairness\\Datasets\"\n",
    "\n",
    "# List all files in the directory\n",
    "files_in_directory = os.listdir(directory_path)\n",
    "print(\"Files in directory:\")\n",
    "for file in files_in_directory:\n",
    "    print(file)\n",
    "\n",
    "# Check if 'adult_p.csv' is in the list\n",
    "if 'adult_p.csv' in files_in_directory:\n",
    "    print(\"File 'adult_p.csv' is visible in the directory.\")\n",
    "else:\n",
    "    print(\"File 'adult_p.csv' is NOT visible in the directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "MqgZi60jt9JD",
    "outputId": "12da7f02-5af4-42b3-e3fd-15010b3444ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fedev\\Downloads\\ParTree-main\\ParTree-main\\Competitors\\algorithmic fairness\\Ablation Study\\Impact of center order\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education_num</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.019622</td>\n",
       "      <td>-0.022667</td>\n",
       "      <td>-0.680470</td>\n",
       "      <td>0.725976</td>\n",
       "      <td>0.094976</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.296610</td>\n",
       "      <td>-0.787368</td>\n",
       "      <td>-0.357412</td>\n",
       "      <td>0.402068</td>\n",
       "      <td>-0.051703</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.083487</td>\n",
       "      <td>-0.069366</td>\n",
       "      <td>0.479827</td>\n",
       "      <td>-0.822414</td>\n",
       "      <td>-0.285691</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.636832</td>\n",
       "      <td>-0.021345</td>\n",
       "      <td>0.256530</td>\n",
       "      <td>-0.721425</td>\n",
       "      <td>-0.087912</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.393080</td>\n",
       "      <td>-0.017952</td>\n",
       "      <td>0.713519</td>\n",
       "      <td>0.574969</td>\n",
       "      <td>-0.073938</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>-0.642387</td>\n",
       "      <td>-0.149353</td>\n",
       "      <td>0.484008</td>\n",
       "      <td>0.564430</td>\n",
       "      <td>-0.110399</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>0.183154</td>\n",
       "      <td>-0.062405</td>\n",
       "      <td>-0.590824</td>\n",
       "      <td>-0.739884</td>\n",
       "      <td>-0.257021</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>0.927796</td>\n",
       "      <td>-0.023090</td>\n",
       "      <td>-0.233823</td>\n",
       "      <td>-0.273761</td>\n",
       "      <td>-0.095099</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>-0.577725</td>\n",
       "      <td>-0.786633</td>\n",
       "      <td>0.052733</td>\n",
       "      <td>-0.199630</td>\n",
       "      <td>-0.069348</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>0.416587</td>\n",
       "      <td>-0.015003</td>\n",
       "      <td>0.393786</td>\n",
       "      <td>-0.177885</td>\n",
       "      <td>0.799700</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age    fnlwgt  education_num  capital_gain  hours_per_week  type\n",
       "0      0.019622 -0.022667      -0.680470      0.725976        0.094976     0\n",
       "1      0.296610 -0.787368      -0.357412      0.402068       -0.051703     0\n",
       "2     -0.083487 -0.069366       0.479827     -0.822414       -0.285691     0\n",
       "3      0.636832 -0.021345       0.256530     -0.721425       -0.087912     0\n",
       "4     -0.393080 -0.017952       0.713519      0.574969       -0.073938     1\n",
       "...         ...       ...            ...           ...             ...   ...\n",
       "32556 -0.642387 -0.149353       0.484008      0.564430       -0.110399     1\n",
       "32557  0.183154 -0.062405      -0.590824     -0.739884       -0.257021     0\n",
       "32558  0.927796 -0.023090      -0.233823     -0.273761       -0.095099     1\n",
       "32559 -0.577725 -0.786633       0.052733     -0.199630       -0.069348     0\n",
       "32560  0.416587 -0.015003       0.393786     -0.177885        0.799700     1\n",
       "\n",
       "[32561 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "def load_Adult(data_dir=''):\n",
    "\n",
    "    data_dir = data_dir\n",
    "    _path = 'adult_p.csv'\n",
    "    data_path = os.path.join(data_dir, _path)\n",
    "\n",
    "    K = 10\n",
    "\n",
    "    df = pandas.read_csv(data_path, sep=',')\n",
    "    #print(df.head())\n",
    "    #print(len(df))\n",
    "    \n",
    "    return df\n",
    "load_Adult(\"C:/Users/fedev/Downloads/ParTree-main/ParTree-main/Competitors/algorithmic fairness/Datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4GF-oe8vt9JD",
    "outputId": "1d0a599f-af46-4d6b-a99d-0e21c9f2df45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32561\n",
      "32561\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'gender'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'gender'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#df['type'] = df['type']-1\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m typ \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgender\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#print(len(typ))\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#print(df.head(10))\u001b[39;00m\n\u001b[0;32m     10\u001b[0m c1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcount_nonzero(typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'gender'"
     ]
    }
   ],
   "source": [
    "df=load_Adult(\"C:/Users/fedev/Downloads/ParTree-main/ParTree-main/Competitors/algorithmic fairness/Datasets\")\n",
    "df= df.round(decimals=5)\n",
    "print(len(df))\n",
    "df = df.dropna()\n",
    "print(len(df))\n",
    "#df['type'] = df['type']-1\n",
    "typ = df['gender'].values\n",
    "#print(len(typ))\n",
    "#print(df.head(10))\n",
    "c1 = np.count_nonzero(typ == 0)\n",
    "c2 = np.count_nonzero(typ == 1)\n",
    "\n",
    "print(c1/(c1+c2))\n",
    "print(c2/(c1+c2))\n",
    "\n",
    "print(c1)\n",
    "print(c2)\n",
    "dfDropped = df.drop(columns=['gender'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "0nVD6Ciot9JF"
   },
   "outputs": [],
   "source": [
    "def dual_print(f,*args,**kwargs):\n",
    "    #print(*args,**kwargs)\n",
    "    print(*args,**kwargs,file=f)\n",
    "\n",
    "def load_dataset(csv_name):\n",
    "    # read the dataset from csv_name and return as pandas dataframe\n",
    "    df = pd.read_csv(csv_name, header=None)\n",
    "    return df\n",
    "\n",
    "\n",
    "def k_random_index(df,K):\n",
    "    # return k random indexes in range of dataframe\n",
    "    return random.sample(range(0, len(df)), K)\n",
    "\n",
    "\n",
    "def find_k_initial_centroid(df,K):\n",
    "    centroids = []    # make of form [ [x1,y1]....]\n",
    "\n",
    "    rnd_idx = k_random_index(df,K)\n",
    "    #print(rnd_idx)\n",
    "    for i in rnd_idx:\n",
    "        coordinates =[]\n",
    "        for a in range(0,A):\n",
    "            coordinates.append(df.loc[i][a])\n",
    "        centroids.append(coordinates)   #df is X,Y,....., Type\n",
    "\n",
    "    return centroids\n",
    "\n",
    "#nOt using\n",
    "def calc_distance(x1, y1, x2, y2):\n",
    "    # returns the euclidean distance between two points\n",
    "    return ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5\n",
    "\n",
    "def calc_distance_a(centroid, point):\n",
    "    #print('çalculating distance\\n')\n",
    "\n",
    "    sum_ = 0\n",
    "\n",
    "    for i in range(0, len(centroid)):\n",
    "        sum_ = sum_ + (centroid[i]-point[i])**2\n",
    "\n",
    "    return sum_ #**0.5\n",
    "\n",
    "@njit(parallel=False)\n",
    "def find_distances_fast(k_centroids, df):\n",
    "    dist = np.zeros((len(k_centroids),len(df),A+2),np.float64)\n",
    "    Kcnt = 0 \n",
    "    for c in k_centroids:  #K-centroid is of form [ c1=[x1,y1.....z1], c2=[x2,y2....z2].....]\n",
    "        l = np.zeros((len(df),A+2),np.float64)\n",
    "        \n",
    "        index = 0 \n",
    "        for row in df:                # row is now x,y,z......type\n",
    "            # append all coordinates to point\n",
    "            dis = np.sum((c- row[:A])**2)#calc_distance_a(c, point)\n",
    "            #Processing the vector for list\n",
    "            row_list = np.array([dis])\n",
    "            #append distance or l norm\n",
    "            row_list = np.append(row_list,row[:A+1])\n",
    "            #append all coordinates #append type of this row\n",
    "  \n",
    "            l[index] = row_list\n",
    "            index = index + 1\n",
    "            #[dist, X, Y,....Z , type]\n",
    "            # l contains list of type [dist,X,Y.....,Z,type] for each points in metric space\n",
    "        dist[Kcnt]= l\n",
    "        Kcnt = Kcnt + 1\n",
    "\n",
    "    # return dist which contains distances of all points from every centroid\n",
    "\n",
    "    return dist\n",
    "\n",
    "def find_distances(k_centroids, df):\n",
    "    dist = []\n",
    "    for c in k_centroids:  #K-centroid is of form [ c1=[x1,y1.....z1], c2=[x2,y2....z2].....]\n",
    "        l = []\n",
    "       # for row in df:\n",
    "\n",
    "        for index, row in df.iterrows():                # row is now x,y,z......type\n",
    "            point =[]\n",
    "            for a in range(0, A):\n",
    "                point.append(row.iloc[a])  # append all coordinates\n",
    "\n",
    "            dis = calc_distance_a(c, point)\n",
    "            #Processing the vector for list\n",
    "            row_list = [dis]\n",
    "            #append distance or l norm\n",
    "\n",
    "            for a in range(0, A):\n",
    "                row_list.append(row.iloc[a])   #append all coordinates\n",
    "            #print(row.iloc[a+1])\n",
    "            row_list.append(row.iloc[a+1])   #append type of this row\n",
    "\n",
    "            l.append(row_list)\n",
    "            #l.append([calc_distance(c[0], c[1], row[0], row[1]), row[0], row[1], row[2]])  # [dist, X, Y,....Z , type]\n",
    "            # l contains list of type [dist,X,Y.....,Z,type] for each points in metric space\n",
    "        dist.append(l)\n",
    "\n",
    "    # return dist which contains distances of all points from every centroid\n",
    "\n",
    "    return dist\n",
    "\n",
    "\n",
    "def sort_and_valuation(dist):\n",
    "    sorted_val = []\n",
    "    for each_centroid_list in dist:\n",
    "        each_centroid_list_sorted = sorted(each_centroid_list, key=lambda x: (x[A+1], x[0]))  # A+1 is index of type , 0 is dist\n",
    "        sorted_val.append(each_centroid_list_sorted)\n",
    "\n",
    "        # sort on basis of type & then dist.\n",
    "        # Now all whites are towards start and all black are after white as they have additional V added to their valuation\n",
    "        # Among the whites, the most closest is at start of list as it has more valuation.\n",
    "        # Similarly sort the black points among them based on distance as did with white\n",
    "\n",
    "    return sorted_val\n",
    "\n",
    "\n",
    "def clustering(sorted_valuation, hashmap_points,K):\n",
    "    n = len(hashmap_points.keys())  # total number of points in metric space\n",
    "     \n",
    "    cluster_assign = []\n",
    "\n",
    "    for i in range(0, K):\n",
    "        cluster_assign.append([])  # initially all clusters are empty\n",
    "    map_index_cluster = []\n",
    "    for i in range(0,K+2):\n",
    "        map_index_cluster.append(0)\n",
    "        #initially check all sorted evaluation from 0th index \n",
    "    number_of_point_alloc = 0\n",
    "    curr_cluster = 0\n",
    "    # until all points are allocated\n",
    "    while number_of_point_alloc != n:  # As convergence is guaranteed that all points will be allocated to some cluster set\n",
    "        start_inde = map_index_cluster[curr_cluster % K]\n",
    "        \n",
    "        for inde in range(start_inde,len(sorted_valuation[curr_cluster % K])):\n",
    "            each = sorted_valuation[curr_cluster % K][inde]\n",
    "            # each is (dist,X,Y,....Z,type)\n",
    "            if hashmap_points[tuple(each[1: -1])] == 0:    # each is (dist, X,Y,....Z, type)\n",
    "                cluster_assign[curr_cluster].append(each)\n",
    "                hashmap_points[tuple(each[1: -1])] = 1\n",
    "                number_of_point_alloc += 1\n",
    "                map_index_cluster[curr_cluster % K] = inde  #next time start from here as isse prev all allocated\n",
    "                break\n",
    "\n",
    "        curr_cluster = (curr_cluster + 1) % K\n",
    "\n",
    "    return cluster_assign\n",
    "\n",
    "\n",
    "def update_centroids_median(cluster_assign,K):\n",
    "    new_centroids = []\n",
    "    for k in range(0, K):\n",
    "        cAk =  np.array(cluster_assign[k])\n",
    "        cAk = np.delete(cAk,[0,-1],axis=1)\n",
    "        if len(cAk) %2 ==0 and len(cAk)>0: \n",
    "            cc = [np.median(np.array(cAk[:-1])[:,cl]) for cl in range(0,cAk.shape[1])]\n",
    "            new_centroids.append(cc)\n",
    "        elif len(cAk) %2 !=0 and len(cAk)>0:\n",
    "            cc = [np.median(np.array(cAk)[:,cl]) for cl in range(0,cAk.shape[1])]\n",
    "            new_centroids.append(cc)\n",
    "        elif len(cAk)==0:\n",
    "            print(\"Error: No centroid found updation error\")\n",
    "    return new_centroids\n",
    "        \n",
    "\n",
    "\n",
    "def update_centroids(cluster_assign,K):\n",
    "\n",
    "    new_centroids = []\n",
    "    for k in range(0, K):\n",
    "\n",
    "        sum_a = []\n",
    "\n",
    "        for i in range(0, A):\n",
    "            sum_a.append(0)\n",
    "\n",
    "        for each in cluster_assign[k]:\n",
    "            sum_a = [sum(x) for x in zip(sum_a, each[1:-1])]\n",
    "            #each is (dist,X,Y,.....Z,type)\n",
    "        new_coordinates = []\n",
    "        for a in range(0, A):\n",
    "            new_coordinates.append(sum_a[a] / len(cluster_assign[k]))\n",
    "        new_centroids.append(new_coordinates)\n",
    "        k=k+1\n",
    "\n",
    "\n",
    "\n",
    "    return new_centroids\n",
    "\n",
    "\n",
    "def calc_clustering_objective(k_centroid, cluster_assign,K):\n",
    "    cost = 0\n",
    "\n",
    "    for k in range(0, K):\n",
    "\n",
    "        for each in cluster_assign[k]:  #each is (dist, X,Y,....,Z,type)\n",
    "            dd = calc_distance_a(k_centroid[k], each[1:-1])\n",
    "            cost = cost + (dd)\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "def calc_fairness_error(df, cluster_assign,K):\n",
    "    U = []  # distribution of each type in original target dataset for each J = 0 , 1....\n",
    "    P_k_sum_over_j = []  # distribution in kth cluster  sum_k( sum_j(   Uj * j wale/total_in_cluster ) )\n",
    "\n",
    "    f_error = 0\n",
    "    cnt_j_0 = 0\n",
    "    cnt_j_1 = 0\n",
    "  #  cnt_j_2 = 0\n",
    "    cnt = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if row.iloc[-1] == 1:\n",
    "            cnt_j_1 += 1\n",
    "        elif row.iloc[-1] == 0:\n",
    "            cnt_j_0 += 1\n",
    "      #  elif row.iloc[-1] == 2:\n",
    "        #    cnt_j_2 += 1\n",
    "            \n",
    "        cnt += 1\n",
    "\n",
    "    U.append(cnt_j_0 / cnt)\n",
    "    U.append(cnt_j_1 / cnt)\n",
    "    #U.append(cnt_j_2 / cnt)\n",
    "\n",
    "   \n",
    "\n",
    "    for k in range(0, K):  # for each cluster\n",
    "\n",
    "        for j in range(0, len(U)):   #for each demographic group\n",
    "\n",
    "            cnt_j_cluster = 0\n",
    "            cnt_total = 0\n",
    "\n",
    "            for each in cluster_assign[k]:\n",
    "                if int(each[-1]) == j:    #each is (dist,X, Y.....,Z,type)\n",
    "                    cnt_j_cluster += 1\n",
    "                cnt_total += 1\n",
    "                \n",
    "            if cnt_j_cluster !=0 and cnt_total != 0:\n",
    "                P_k_sum_over_j.append(-U[j] * np.log((cnt_j_cluster / cnt_total)/U[j]))\n",
    "            else:\n",
    "                P_k_sum_over_j.append(0)  #log(0)=0 considered\n",
    "\n",
    "    for each in P_k_sum_over_j:\n",
    "        f_error += each\n",
    "\n",
    "    return f_error\n",
    "\n",
    "\n",
    "def calc_balance(cluster_assign,K):\n",
    "    S_k = []  # balance of each k cluster\n",
    "    balance = 0  # min (S_k)\n",
    "\n",
    "    for k in range(0, K):\n",
    "        cnt_j_0 = 0\n",
    "        cnt_j_1 = 0\n",
    "       # cnt_j_2 = 0\n",
    "        cnt = 0\n",
    "        for each in cluster_assign[k]:\n",
    "\n",
    "            if int(each[-1]) == 1:\n",
    "                cnt_j_1 += 1\n",
    "            elif int(each[-1]) == 0:\n",
    "                cnt_j_0 += 1\n",
    "           # elif int(each[-1]) == 2:\n",
    "           #     cnt_j_2 += 1\n",
    "                \n",
    "            cnt += 1\n",
    "\n",
    "        if cnt_j_0 != 0 and cnt_j_1 != 0 :#and cnt_j_2!= 0:\n",
    "            S_k.append(min([cnt_j_0 / cnt_j_1, cnt_j_1 / cnt_j_0 ]))#, cnt_j_1 / cnt_j_2 , cnt_j_2 / cnt_j_1 , cnt_j_0 / cnt_j_2, cnt_j_2 / cnt_j_0 ]))\n",
    "        elif cnt_j_0 == 0 or cnt_j_1 ==0  :#or cnt_j_2==0:\n",
    "            S_k.append(0)\n",
    "\n",
    "\n",
    "\n",
    "    balance = min(S_k)\n",
    "\n",
    "    return balance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "uBwSXAMnt9JG"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    # Step1 : Load the dataset\n",
    "            \n",
    "    list_fair_K=[]\n",
    "    list_obj_K =[]       \n",
    "    list_balance_K=[]\n",
    "    \n",
    "    os.makedirs('Adult_kmeans_permu')\n",
    "    \n",
    "    for kk in [10]:#2,5,10,15,20,30,40]:\n",
    "        K = kk\n",
    "        \n",
    "        print(\" K==\"+str(K)+\"  \")\n",
    "        \n",
    "        list_fair_run=[]\n",
    "        list_obj_run =[]       \n",
    "        list_balance_run=[]\n",
    "        seeds = [0,100,200,300,400,500,600,700,800,900,1000,1100]\n",
    "      \n",
    "        for run in range(0,runs):\n",
    "            np.random.seed(seeds[run])\n",
    "            random.seed(seeds[run])\n",
    "            f = open('Adult_kmeans_permu/K_'+str(K)+'_run_'+str(run)+'_output.txt', 'a')\n",
    "            print(\"+\"*100)\n",
    "            print('                        RUN  : '+ str(run))\n",
    "\n",
    "\n",
    "            list_fair_iter=[]\n",
    "            list_obj_iter =[]\n",
    "            list_balance_iter=[]\n",
    "\n",
    "            # Step2 : Find initial K random centroids using k_random_index(df) & find_k_initial_centroid(df)\n",
    "            k_centroid= find_k_initial_centroid(df,kk)\n",
    "            k_centroid_permu = list(permutations(k_centroid))\n",
    "            random.shuffle(k_centroid_permu)\n",
    "            k_centroid = k_centroid_permu[0]\n",
    "            permu_index = 0\n",
    "            max_permu_len =100 #len(k_centroid_permu)\n",
    "            print(\"Number of Permutations : \"+str(max_permu_len))\n",
    "            \n",
    "            prev_assignment =[]\n",
    "            cluster_assignment = []\n",
    "\n",
    "            for i in range(0, K):\n",
    "                cluster_assignment.append([])  # initially all clusters are empty\n",
    "\n",
    "            sum_time = 0\n",
    "            curr_itr = 0\n",
    "            prev_objective_cost=-1\n",
    "            objective_cost = 0\n",
    "                # Step3 : Find distances from the centroids using find_distances() with list of [ [x1,y1,z1..] , [x2,y2,z2..]....] centroids format list\n",
    "            while True:# and prev_objective_cost != objective_cost:\n",
    "\n",
    "                start = time.process_time()#timeit.default_timer()\n",
    "          \n",
    "                dual_print(f,'Calulating distance for iteration : '+ str(curr_itr)+'\\n')\n",
    "                df1 = df.values\n",
    "                k_centroids1= np.array(k_centroid)\n",
    "\n",
    "                dist = find_distances_fast(k_centroids1, df1)\n",
    "                dual_print(f,'Finished calc distance for iteration : '+ str(curr_itr)+'\\n')\n",
    "                # Step4 :  Find Valuation matrix for all centroids using sort_and_valuation()\n",
    "                dual_print(f,'Calulating Valuation for iteration : '+ str(curr_itr)+'\\n')\n",
    "\n",
    "                valuation = sort_and_valuation(dist)\n",
    "                dual_print(f,'Finished Valuation for iteration : '+ str(curr_itr)+'\\n')\n",
    "\n",
    "                #Step5 : Perform clustering using valuation matrix & hashmap of all points in metric\n",
    "                hash_map = {}\n",
    "                for index, row in df.iterrows():\n",
    "                    temp = tuple(row[:-1])\n",
    "                    hash_map.update({tuple(row[:-1]): 0})   #dict is of form { (x,y): 0 , ....}\n",
    "                dual_print(f,'Finding clusters for iteration : '+ str(curr_itr)+'\\n')\n",
    "                prev_assignment = cluster_assignment\n",
    "                cluster_assignment = clustering(valuation, hash_map,K)\n",
    "                \n",
    "                print(\"cluster_assignment\", cluster_assignment)\n",
    "\n",
    "                dual_print(f,'Finished finding cluster for iteration : '+ str(curr_itr)+'\\n')\n",
    "\n",
    "              #  print(\"Finding balance \")\n",
    "                balance = calc_balance(cluster_assignment,K)\n",
    "                f_error = calc_fairness_error(df, cluster_assignment,K)\n",
    "                clustering_cost = calc_clustering_objective(k_centroid,cluster_assignment,K)\n",
    "                    \n",
    "                objective_cost = np.round(clustering_cost,3)\n",
    "\n",
    "\n",
    "              \n",
    "                list_balance_iter.append(str(balance))\n",
    "                list_obj_iter.append(str(objective_cost))\n",
    "                list_fair_iter.append(str(f_error))\n",
    "\n",
    "                dual_print(f,'balance : ' + str(balance) + '\\n')\n",
    "                dual_print(f,'Fairness Error : ' + str(f_error) + '\\n')\n",
    "                dual_print(f,'Clustering Objective/Cost ' + str(clustering_cost) + '\\n')\n",
    "\n",
    "                #Step6 : Print the cluster assignments\n",
    "                #Step7 : Find new centroids using mean of all points in current assignment\n",
    "                \n",
    "                stopFlag =0\n",
    "                if permu_index < max_permu_len-1:\n",
    "                    permu_index += 1\n",
    "                    k_centroid = k_centroid_permu[permu_index] #update_centroids(cluster_assignment,K)\n",
    "                else:\n",
    "                    stopFlag =1\n",
    "                \n",
    "                \n",
    "                dual_print(f,'Finished centroid updation for iteration : '+ str(curr_itr)+'\\n')\n",
    "                dual_print(f,'Iteration No: '+str(curr_itr)+' : updated centroid are : '+ str(k_centroid))\n",
    "                #Step8 : Repeat from Step3 until clusters are same or iterations reach upper limit\n",
    "                stop = time.process_time()#timeit.default_timer()\n",
    "                sum_time += (stop - start)\n",
    "                dual_print(f,'Time for iteration : ' + str(curr_itr) + ' is  ' + str(stop - start) + '\\n')\n",
    "                \n",
    "            \n",
    "                curr_itr += 1\n",
    "                \n",
    "                if stopFlag==1:\n",
    "                    break\n",
    "                    \n",
    "                dual_print(f,'-----------------------------Finished-----------------------------------------------\\n')\n",
    "\n",
    "\n",
    "              \n",
    "            print('Total time taken to converge '+ str(sum_time)+'\\n')\n",
    "            print('Iterations total taken for convergence : '+str(curr_itr)+'\\n')\n",
    "\n",
    "            dual_print(f,'Total time taken is '+ str(sum_time)+'\\n')\n",
    "            dual_print(f,'Iterations total : '+str(curr_itr-1))\n",
    "           \n",
    "            #Step 10 : Find balance , fairness error , and clustering objective or cost\n",
    "\n",
    "            balance_converged = calc_balance(cluster_assignment,K)\n",
    "            f_error_converged = calc_fairness_error(df, cluster_assignment,K)\n",
    "            clustering_cost_converged = calc_clustering_objective(k_centroid,cluster_assignment,K)\n",
    "\n",
    "            print(\"\\nCost variation over iterations\")\n",
    "            print(list_obj_iter)\n",
    "            print(\"\\nBalance variation over iterations\")\n",
    "            print(list_balance_iter)\n",
    "            print(\"\\nFairness error over iterations\")\n",
    "            print(list_fair_iter)\n",
    "            print('\\n')\n",
    "            print('Final converged balance : ' + str(balance_converged) + '\\n')\n",
    "            print('Final Converged Fairness Error : ' + str(f_error_converged) + '\\n')\n",
    "            print('Final converged Clustering Objective/Cost ' + str(clustering_cost_converged) + '\\n')\n",
    "\n",
    "            dual_print(f,'Converged balance : ' + str(balance_converged) + '\\n')\n",
    "            dual_print(f,'Converged Fairness Error : ' + str(f_error_converged) + '\\n')\n",
    "\n",
    "            dual_print(f,'Converged Clustering Objective/Cost ' + str(clustering_cost_converged) + '\\n')\n",
    "            \n",
    "            f.close()\n",
    "            run  = run +1\n",
    "            list_obj_run.append(clustering_cost_converged)\n",
    "            list_fair_run.append(f_error_converged)\n",
    "            list_balance_run.append(balance_converged)\n",
    "        \n",
    "        print(\"@\"*70)\n",
    "        print(\"Cost variations over run\")\n",
    "        print(str(list_obj_run))\n",
    "        print(\"balance variations over run\")\n",
    "        print(str(list_balance_run))\n",
    "        print(\"fairness error over run\")\n",
    "        print(str(list_fair_run))\n",
    "        print(\"#\"*30)\n",
    "        print(\"Mean Cost variations over run\")\n",
    "        print(str(np.mean(np.array(list_obj_run))))\n",
    "        print(\"Std Dev Cost variations over run\")\n",
    "        print(str(np.std(np.array(list_obj_run))))\n",
    "        print(\"#\"*30)\n",
    "        \n",
    "        list_obj_K.append(np.mean(np.array(list_obj_run)))\n",
    "        list_fair_K.append(np.mean(np.array(list_fair_run)))\n",
    "        list_balance_K.append(np.mean(np.array(list_balance_run)))\n",
    "        \n",
    "    print(\"%\"*70)\n",
    "    print(\"Cost variations over K\")\n",
    "    print(str(list_obj_K))\n",
    "    print(\"balance variations over K\")\n",
    "    print(str(list_balance_K))\n",
    "    print(\"fairness error over K\")\n",
    "    print(str(list_fair_K))\n",
    "    print(\"#\"*30)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "yQGBVfEIt9JH",
    "outputId": "4074e1ca-f909-4a76-9aff-3b964700c279",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Impossibile creare un file, se il file esiste già: 'Adult_kmeans_permu'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 3\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[23], line 8\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m list_obj_K \u001b[38;5;241m=\u001b[39m[]       \n\u001b[0;32m      6\u001b[0m list_balance_K\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m----> 8\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdult_kmeans_permu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m kk \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m10\u001b[39m]:\u001b[38;5;66;03m#2,5,10,15,20,30,40]:\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     K \u001b[38;5;241m=\u001b[39m kk\n",
      "File \u001b[1;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Impossibile creare un file, se il file esiste già: 'Adult_kmeans_permu'"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EF1_adult_KmeansPermu_10_new.ipynb",
   "provenance": [
    {
     "file_id": "1Uco7HsEbyMqUWxcyMzPZ-2CmLuJQDSvW",
     "timestamp": 1628609962874
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
